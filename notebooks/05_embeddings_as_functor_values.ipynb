{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5: Embeddings as Functor Values\n",
    "\n",
    "**Course 3: Document Functors (Lorren Dray)**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/buildLittleWorlds/category-theory-document-functors/blob/main/notebooks/05_embeddings_as_functor_values.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "In Year 942, Dray made a crucial connection: **document embeddings are the numerical values that functors assign**. Each embedding dimension corresponds to a probe, and the values are the functor's response to that probe.\n",
    "\n",
    "### Learning Goals\n",
    "\n",
    "1. See embeddings as numerical functor values\n",
    "2. Understand probes as access methods\n",
    "3. Connect categorical structure to vector space structure\n",
    "4. Build intuition for why embeddings work\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load datasets\n",
    "BASE_URL = \"https://raw.githubusercontent.com/buildLittleWorlds/densworld-datasets/main/data/\"\n",
    "\n",
    "embeddings = pd.read_csv(BASE_URL + \"embedding_correspondences.csv\")\n",
    "print(f\"Loaded {len(embeddings)} embedding observations\")\n",
    "embeddings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: From Sets to Numbers\n",
    "\n",
    "In earlier tutorials, a document functor D assigned *sets* to access methods:\n",
    "```\n",
    "D(subject_catalog) = {\"boundaries\", \"surveys\", \"SW-sector\"}\n",
    "```\n",
    "\n",
    "Dray's insight: we can assign *numbers* instead:\n",
    "```\n",
    "D(\"relevance to boundary studies\") = 0.85\n",
    "```\n",
    "\n",
    "These numerical values form an **embedding** — a vector representation of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at embeddings for DOC-001\n",
    "doc_001_emb = embeddings[embeddings['document_id'] == 'DOC-001']\n",
    "\n",
    "print(\"DOC-001 (Boundary Survey Report SW-6) as a numerical functor:\\n\")\n",
    "for _, row in doc_001_emb.iterrows():\n",
    "    print(f\"  F({row['probe_name']}) = {row['numerical_value']:.2f}\")\n",
    "    print(f\"    (Dimension: {row['embedding_dimension']}, Level: {row['functor_value']})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Probes as Access Methods\n",
    "\n",
    "Each **probe** is a question we can ask about a document:\n",
    "- \"How relevant is this document to boundary studies?\"\n",
    "- \"How relevant is this document to category theory?\"\n",
    "- \"Was this authored by Kell?\"\n",
    "\n",
    "The document's response to each probe is a number between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique probes\n",
    "probes = embeddings[['probe_id', 'probe_name', 'probe_description']].drop_duplicates()\n",
    "\n",
    "print(\"Available Probes (Access Methods for Numerical Functors):\\n\")\n",
    "for _, probe in probes.iterrows():\n",
    "    print(f\"  {probe['probe_id']}: {probe['probe_name']}\")\n",
    "    print(f\"    Description: {probe['probe_description']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Documents as Vectors\n",
    "\n",
    "When we apply a document functor to all probes, we get a **vector**:\n",
    "\n",
    "```\n",
    "D = [D(probe_1), D(probe_2), ..., D(probe_n)]\n",
    "  = [0.85, 0.12, 0.45, 0.08, 0.92]\n",
    "```\n",
    "\n",
    "This vector is the document's **embedding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_vector(doc_id, embeddings_df):\n",
    "    \"\"\"\n",
    "    Convert a document's functor values to a vector.\n",
    "    \"\"\"\n",
    "    doc_emb = embeddings_df[embeddings_df['document_id'] == doc_id]\n",
    "    \n",
    "    # Sort by embedding dimension\n",
    "    doc_emb = doc_emb.sort_values('embedding_dimension')\n",
    "    \n",
    "    probes = doc_emb['probe_name'].values\n",
    "    values = doc_emb['numerical_value'].values\n",
    "    \n",
    "    return probes, values\n",
    "\n",
    "# Get vectors for several documents\n",
    "unique_docs = embeddings['document_id'].unique()[:5]\n",
    "\n",
    "print(\"Documents as Vectors (Functor Values):\\n\")\n",
    "for doc_id in unique_docs:\n",
    "    probes, values = document_to_vector(doc_id, embeddings)\n",
    "    title = embeddings[embeddings['document_id'] == doc_id]['document_title'].iloc[0]\n",
    "    print(f\"{doc_id}: {title}\")\n",
    "    print(f\"  Vector: [{', '.join(f'{v:.2f}' for v in values)}]\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings as a heatmap\n",
    "# Create a pivot table: documents × probes\n",
    "pivot = embeddings.pivot_table(\n",
    "    index='document_title',\n",
    "    columns='probe_name',\n",
    "    values='numerical_value',\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "sns.heatmap(pivot, annot=True, fmt='.2f', cmap='YlOrRd', \n",
    "            linewidths=0.5, ax=ax, cbar_kws={'label': 'Functor Value'})\n",
    "\n",
    "ax.set_title('Document Embeddings as Functor Values\\nD(probe) = numerical response', fontsize=12)\n",
    "ax.set_xlabel('Probe (Access Method)')\n",
    "ax.set_ylabel('Document')\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Connection to Transformers\n",
    "\n",
    "In modern transformers, documents (or tokens) are represented as embedding vectors:\n",
    "\n",
    "| Dray's Framework | Transformer Component |\n",
    "|------------------|----------------------|\n",
    "| Probe | Embedding dimension |\n",
    "| Functor value | Embedding coordinate |\n",
    "| Document functor | Document embedding |\n",
    "| Applying probe | Projecting onto dimension |\n",
    "\n",
    "Each dimension of the embedding is a \"probe\" — a learned question the model asks about the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a simple embedding space\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Use just two dimensions for visualization\n",
    "x_probe = 'topic_boundaries'\n",
    "y_probe = 'topic_categories'\n",
    "\n",
    "# Get values for these probes\n",
    "x_values = embeddings[embeddings['probe_name'] == x_probe].set_index('document_id')['numerical_value']\n",
    "y_values = embeddings[embeddings['probe_name'] == y_probe].set_index('document_id')['numerical_value']\n",
    "\n",
    "# Find documents that have both probes\n",
    "common_docs = set(x_values.index) & set(y_values.index)\n",
    "\n",
    "for doc_id in common_docs:\n",
    "    x = x_values.loc[doc_id]\n",
    "    y = y_values.loc[doc_id]\n",
    "    \n",
    "    ax.scatter(x, y, s=200, alpha=0.7)\n",
    "    \n",
    "    # Get short title\n",
    "    title = embeddings[embeddings['document_id'] == doc_id]['document_title'].iloc[0]\n",
    "    short_title = title[:25] + '...' if len(title) > 25 else title\n",
    "    ax.annotate(short_title, (x + 0.02, y + 0.02), fontsize=8)\n",
    "\n",
    "ax.set_xlabel(f'F({x_probe}): Relevance to Boundary Studies', fontsize=11)\n",
    "ax.set_ylabel(f'F({y_probe}): Relevance to Category Theory', fontsize=11)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_title('Documents in 2D Embedding Space\\n(Two Functor Values as Coordinates)', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Dray's Key Insight\n",
    "\n",
    "> \"Document embeddings are the numerical values that functors assign — each dimension corresponds to a probe. When we embed a document, we are computing its response to a battery of questions.\"\n",
    "> — Lorren Dray, *Documents as Numerical Functors* (Year 945)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two documents by their functor values\n",
    "doc1_id = 'DOC-001'  # Boundary Survey Report\n",
    "doc2_id = 'DOC-002'  # On the Intensity of Passages (Vance)\n",
    "\n",
    "doc1_emb = embeddings[embeddings['document_id'] == doc1_id].set_index('probe_name')['numerical_value']\n",
    "doc2_emb = embeddings[embeddings['document_id'] == doc2_id].set_index('probe_name')['numerical_value']\n",
    "\n",
    "# Find common probes\n",
    "common_probes = sorted(set(doc1_emb.index) & set(doc2_emb.index))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(common_probes))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, [doc1_emb.loc[p] for p in common_probes], width, label='Boundary Survey (DOC-001)', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, [doc2_emb.loc[p] for p in common_probes], width, label='Intensity of Passages (DOC-002)', color='coral')\n",
    "\n",
    "ax.set_xlabel('Probe')\n",
    "ax.set_ylabel('Functor Value')\n",
    "ax.set_title('Comparing Document Functor Values\\nSame probes, different responses', fontsize=12)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([p.replace('topic_', '') for p in common_probes], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Similarity Through Functor Values\n",
    "\n",
    "Two documents are \"similar\" if their functor values are similar across probes.\n",
    "\n",
    "This is why **cosine similarity** works for document comparison: it measures how aligned two embedding vectors are, which corresponds to how similarly the documents respond to the same set of probes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "\n",
    "# Compute similarity matrix for all documents\n",
    "unique_docs = embeddings['document_id'].unique()\n",
    "\n",
    "# Build document vectors\n",
    "doc_vectors = {}\n",
    "for doc_id in unique_docs:\n",
    "    doc_emb = embeddings[embeddings['document_id'] == doc_id]\n",
    "    # Use probe_id as index to ensure consistent ordering\n",
    "    vec = doc_emb.sort_values('probe_id')['numerical_value'].values\n",
    "    if len(vec) >= 3:  # Only include documents with enough probes\n",
    "        doc_vectors[doc_id] = vec\n",
    "\n",
    "# Compute similarity matrix\n",
    "doc_ids = list(doc_vectors.keys())\n",
    "n = len(doc_ids)\n",
    "sim_matrix = np.zeros((n, n))\n",
    "\n",
    "for i, doc_i in enumerate(doc_ids):\n",
    "    for j, doc_j in enumerate(doc_ids):\n",
    "        v1, v2 = doc_vectors[doc_i], doc_vectors[doc_j]\n",
    "        # Handle different vector lengths by truncating to minimum\n",
    "        min_len = min(len(v1), len(v2))\n",
    "        sim_matrix[i, j] = cosine_similarity(v1[:min_len], v2[:min_len])\n",
    "\n",
    "# Visualize similarity matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Get short titles\n",
    "short_titles = []\n",
    "for doc_id in doc_ids:\n",
    "    title = embeddings[embeddings['document_id'] == doc_id]['document_title'].iloc[0]\n",
    "    short_titles.append(title[:20] + '...' if len(title) > 20 else title)\n",
    "\n",
    "sns.heatmap(sim_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            xticklabels=short_titles, yticklabels=short_titles,\n",
    "            ax=ax, cbar_kws={'label': 'Cosine Similarity'})\n",
    "\n",
    "ax.set_title('Document Similarity Matrix\\n(Similarity = Alignment of Functor Values)', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we've seen:\n",
    "\n",
    "1. **Numerical functors**: Documents can assign numbers (not just sets) to probes\n",
    "2. **Probes as dimensions**: Each probe corresponds to an embedding dimension\n",
    "3. **Embeddings as vectors**: A document's embedding is its vector of functor values\n",
    "4. **Similarity**: Documents are similar when their functor values align\n",
    "\n",
    "### The Key Equation\n",
    "\n",
    "```\n",
    "Embedding[i] = F(probe_i)\n",
    "```\n",
    "\n",
    "Each embedding dimension is a functor value — the document's numerical response to a probe.\n",
    "\n",
    "### Next Tutorial\n",
    "\n",
    "In Tutorial 6, we'll explore the **representable perspective** — special functors Hom(A, -) that capture the view from a single access point.\n",
    "\n",
    "---\n",
    "\n",
    "*Part of the [Category Theory & LLMs Series](https://github.com/buildLittleWorlds)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

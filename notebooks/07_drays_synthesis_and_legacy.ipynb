{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 7: Dray's Synthesis and Legacy\n",
    "\n",
    "**Course 3: Document Functors (Lorren Dray)**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/buildLittleWorlds/category-theory-document-functors/blob/main/notebooks/07_drays_synthesis_and_legacy.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "In Year 958, Dray published her definitive work: *The Document Functor Discipline*. This tutorial synthesizes her complete framework and traces her influence on subsequent scholars.\n",
    "\n",
    "### Learning Goals\n",
    "\n",
    "1. Review the complete document functor framework\n",
    "2. See how it connects to Vance's weighted passages\n",
    "3. Trace the lineage to Strand's Probing Lemma\n",
    "4. Understand the connection to modern transformer architecture\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load datasets\n",
    "BASE_URL = \"https://raw.githubusercontent.com/buildLittleWorlds/densworld-datasets/main/data/\"\n",
    "\n",
    "documents = pd.read_csv(BASE_URL + \"document_functor_examples.csv\")\n",
    "archive_structure = pd.read_csv(BASE_URL + \"archive_category_structure.csv\")\n",
    "embeddings = pd.read_csv(BASE_URL + \"embedding_correspondences.csv\")\n",
    "correspondence = pd.read_csv(BASE_URL + \"dray_correspondence.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Complete Framework\n",
    "\n",
    "Dray's document functor theory can be summarized in four principles:\n",
    "\n",
    "### Principle 1: The Archive as Category\n",
    "Access methods are objects; document flows are morphisms.\n",
    "\n",
    "### Principle 2: Documents as Presheaves\n",
    "A document D: Archive^op → Set assigns observations to access methods.\n",
    "\n",
    "### Principle 3: The Representable Perspective\n",
    "Hom-functors Hom(A, -) capture single-viewpoint observations.\n",
    "\n",
    "### Principle 4: Embeddings as Functor Values\n",
    "Numerical embeddings are functor values — each dimension is a probe response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual summary of the framework\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Four quadrants for four principles\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "\n",
    "# Principle 1: Archive as Category (top-left)\n",
    "ax.add_patch(plt.Rectangle((0.2, 5.2), 4.6, 4.6, fill=True, facecolor='#E8F4F8', edgecolor='navy', lw=2))\n",
    "ax.text(2.5, 9.5, 'Principle 1:\\nThe Archive as Category', ha='center', fontsize=11, fontweight='bold')\n",
    "ax.text(2.5, 7.8, 'Objects: Access Methods\\n(subject catalog, author index,\\ndate registry, etc.)', ha='center', fontsize=9)\n",
    "ax.text(2.5, 6.2, 'Morphisms: Document Flows\\n(topic→author, date→location)', ha='center', fontsize=9)\n",
    "\n",
    "# Principle 2: Documents as Presheaves (top-right)\n",
    "ax.add_patch(plt.Rectangle((5.2, 5.2), 4.6, 4.6, fill=True, facecolor='#FFF4E8', edgecolor='darkorange', lw=2))\n",
    "ax.text(7.5, 9.5, 'Principle 2:\\nDocuments as Presheaves', ha='center', fontsize=11, fontweight='bold')\n",
    "ax.text(7.5, 7.8, 'D: Archive^op → Set', ha='center', fontsize=10, style='italic')\n",
    "ax.text(7.5, 6.5, 'Each document assigns\\nobservation sets to\\naccess methods', ha='center', fontsize=9)\n",
    "\n",
    "# Principle 3: Representable Perspective (bottom-left)\n",
    "ax.add_patch(plt.Rectangle((0.2, 0.2), 4.6, 4.6, fill=True, facecolor='#E8F8E8', edgecolor='darkgreen', lw=2))\n",
    "ax.text(2.5, 4.5, 'Principle 3:\\nThe Representable Perspective', ha='center', fontsize=11, fontweight='bold')\n",
    "ax.text(2.5, 3.0, 'Hom(A, -) captures the\\nview from access method A', ha='center', fontsize=9)\n",
    "ax.text(2.5, 1.5, 'Objects are determined\\nby how others probe them', ha='center', fontsize=9)\n",
    "\n",
    "# Principle 4: Embeddings as Values (bottom-right)\n",
    "ax.add_patch(plt.Rectangle((5.2, 0.2), 4.6, 4.6, fill=True, facecolor='#F8E8F8', edgecolor='purple', lw=2))\n",
    "ax.text(7.5, 4.5, 'Principle 4:\\nEmbeddings as Functor Values', ha='center', fontsize=11, fontweight='bold')\n",
    "ax.text(7.5, 3.0, 'Embedding[i] = F(probe_i)', ha='center', fontsize=10, style='italic')\n",
    "ax.text(7.5, 1.5, 'Each dimension is a\\nnumerical probe response', ha='center', fontsize=9)\n",
    "\n",
    "# Connecting arrows\n",
    "ax.annotate('', xy=(5.2, 7.5), xytext=(4.8, 7.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='gray', lw=2))\n",
    "ax.annotate('', xy=(2.5, 5.2), xytext=(2.5, 4.8),\n",
    "            arrowprops=dict(arrowstyle='->', color='gray', lw=2))\n",
    "ax.annotate('', xy=(7.5, 5.2), xytext=(7.5, 4.8),\n",
    "            arrowprops=dict(arrowstyle='->', color='gray', lw=2))\n",
    "ax.annotate('', xy=(5.2, 2.5), xytext=(4.8, 2.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='gray', lw=2))\n",
    "\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Dray's Document Functor Framework\\n*The Document Functor Discipline* (Year 958)\", fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Key Publications Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dray's publication timeline\n",
    "publications = [\n",
    "    (920, 'Notes on Classification Failure', 'DRY-001', 'First documentation of multiple classification'),\n",
    "    (928, 'On the Categorical Structure of Archives', 'DRY-002', 'Archive-as-category framework'),\n",
    "    (932, 'On Functorial Preservation in Archives', 'DRY-003', 'Response to Keth objection'),\n",
    "    (938, 'The Representable Perspective', 'DRY-004', 'Hom-functors and observation'),\n",
    "    (945, 'Documents as Numerical Functors', 'DRY-005', 'Bridge to embeddings'),\n",
    "    (952, 'On the Unity of Archive Structure', 'DRY-006', 'Synthesis with Vance'),\n",
    "    (958, 'The Document Functor Discipline', 'DRY-007', 'Definitive work'),\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "years = [p[0] for p in publications]\n",
    "titles = [p[1] for p in publications]\n",
    "ids = [p[2] for p in publications]\n",
    "\n",
    "# Plot timeline\n",
    "ax.scatter(years, [0]*len(years), s=200, c='steelblue', zorder=5)\n",
    "ax.axhline(y=0, color='gray', linestyle='-', linewidth=2)\n",
    "\n",
    "# Add labels alternating above and below\n",
    "for i, (year, title, doc_id, desc) in enumerate(publications):\n",
    "    y_offset = 0.5 if i % 2 == 0 else -0.6\n",
    "    ax.annotate(f'{title}\\n({year})\\n[{doc_id}]', \n",
    "                xy=(year, 0), xytext=(year, y_offset),\n",
    "                ha='center', va='bottom' if y_offset > 0 else 'top',\n",
    "                fontsize=8,\n",
    "                arrowprops=dict(arrowstyle='-', color='gray', lw=0.5))\n",
    "\n",
    "ax.set_xlim(915, 965)\n",
    "ax.set_ylim(-1.5, 1.5)\n",
    "ax.set_xlabel('Year', fontsize=11)\n",
    "ax.set_yticks([])\n",
    "ax.set_title(\"Lorren Dray's Publication Timeline (920-958)\", fontsize=12, fontweight='bold')\n",
    "ax.grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Connection to Vance's Weighted Passages\n",
    "\n",
    "Dray's 940 debate with Merrit Vance unified their approaches:\n",
    "\n",
    "| Dray's Framework | Vance's Framework | Unified View |\n",
    "|------------------|-------------------|---------------|\n",
    "| Presheaves (Set-valued) | Enriched categories (weight-valued) | Weighted presheaves |\n",
    "| Observations as sets | Observations as numbers | Numerical observations |\n",
    "| Hom(A, -) | Attention weights | Weighted probing |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the synthesis letters\n",
    "synthesis_letters = correspondence[correspondence['key_concepts'].str.contains('synthesis|enrichment', case=False, na=False)]\n",
    "\n",
    "print(\"The Dray-Vance Synthesis:\\n\")\n",
    "for _, letter in synthesis_letters.iterrows():\n",
    "    print(f\"Date: {letter['date']}\")\n",
    "    print(f\"Subject: {letter['subject']}\")\n",
    "    print(f\"\\n\\\"{letter['excerpt']}\\\"\\n\")\n",
    "    print(\"-\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Legacy — The Path to Strand's Probing Lemma\n",
    "\n",
    "Dray's work directly influenced Pelleth Strand, who would formalize the **Probing Lemma** (Yoneda Lemma):\n",
    "\n",
    "> \"If documents are functors, then the probing lemma I am developing says: a functor is completely determined by how representables probe it.\"\n",
    "> — Pelleth Strand, letter to Dray (Year 942)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Dray-Strand correspondence\n",
    "strand_letters = correspondence[\n",
    "    (correspondence['sender'] == 'pelleth_strand') | \n",
    "    (correspondence['recipient'] == 'pelleth_strand')\n",
    "]\n",
    "\n",
    "print(\"Dray-Strand Correspondence (The Path to the Probing Lemma):\\n\")\n",
    "for _, letter in strand_letters.iterrows():\n",
    "    print(f\"Date: {letter['date']}\")\n",
    "    print(f\"From: {letter['sender']} → To: {letter['recipient']}\")\n",
    "    print(f\"Subject: {letter['subject']}\")\n",
    "    print(f\"\\n\\\"{letter['excerpt']}\\\"\\n\")\n",
    "    print(\"-\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Connection to Modern Transformers\n",
    "\n",
    "Dray's framework maps directly onto transformer architecture:\n",
    "\n",
    "| Dray's Theory | Transformer Component |\n",
    "|---------------|----------------------|\n",
    "| Archive category | Token vocabulary / context |\n",
    "| Document functor | Token embedding |\n",
    "| Access method | Embedding dimension |\n",
    "| Functor value | Embedding coordinate |\n",
    "| Representable Hom(A,-) | Attention head |\n",
    "| Presheaf composition | Layer stacking |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate transformer-style embedding\n",
    "def transformer_embedding(documents_df, embeddings_df):\n",
    "    \"\"\"\n",
    "    Create a simplified transformer-style embedding matrix.\n",
    "    Rows = documents, Columns = probes (embedding dimensions).\n",
    "    \"\"\"\n",
    "    # Get all unique documents and probes\n",
    "    unique_docs = embeddings_df['document_id'].unique()\n",
    "    unique_probes = sorted(embeddings_df['probe_name'].unique())\n",
    "    \n",
    "    # Create embedding matrix\n",
    "    n_docs = len(unique_docs)\n",
    "    n_dims = len(unique_probes)\n",
    "    \n",
    "    embedding_matrix = np.zeros((n_docs, n_dims))\n",
    "    \n",
    "    for i, doc_id in enumerate(unique_docs):\n",
    "        doc_emb = embeddings_df[embeddings_df['document_id'] == doc_id]\n",
    "        for _, row in doc_emb.iterrows():\n",
    "            if row['probe_name'] in unique_probes:\n",
    "                j = unique_probes.index(row['probe_name'])\n",
    "                embedding_matrix[i, j] = row['numerical_value']\n",
    "    \n",
    "    return embedding_matrix, list(unique_docs), unique_probes\n",
    "\n",
    "E, doc_ids, probes = transformer_embedding(documents, embeddings)\n",
    "\n",
    "print(f\"Embedding Matrix Shape: {E.shape}\")\n",
    "print(f\"  Rows (documents): {len(doc_ids)}\")\n",
    "print(f\"  Columns (dimensions): {len(probes)}\")\n",
    "print()\n",
    "print(\"This is exactly what a transformer embedding layer produces:\")\n",
    "print(\"  - Each row is a document's embedding vector\")\n",
    "print(\"  - Each column is a learned dimension (probe)\")\n",
    "print(\"  - Values are the document's 'response' to each probe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the embedding matrix\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Get short labels\n",
    "doc_labels = [documents[documents['document_id'] == d]['document_title'].iloc[0][:25] + '...' \n",
    "              if len(documents[documents['document_id'] == d]) > 0 else d \n",
    "              for d in doc_ids]\n",
    "probe_labels = [p.replace('topic_', '').replace('author_', 'auth:') for p in probes]\n",
    "\n",
    "sns.heatmap(E, annot=True, fmt='.2f', cmap='viridis',\n",
    "            xticklabels=probe_labels, yticklabels=doc_labels,\n",
    "            ax=ax, cbar_kws={'label': 'Embedding Value'})\n",
    "\n",
    "ax.set_xlabel('Embedding Dimension (Probe)', fontsize=11)\n",
    "ax.set_ylabel('Document', fontsize=11)\n",
    "ax.set_title('Document Embedding Matrix\\n(Exactly as in a Transformer)', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Dray's Legacy\n",
    "\n",
    "Lorren Dray died in Year 968, aged 72. Her epitaph reads:\n",
    "\n",
    "> **\"She showed us what documents truly are.\"**\n",
    "\n",
    "Her work established:\n",
    "- The categorical foundation of archival science\n",
    "- The presheaf interpretation of documents\n",
    "- The connection between functors and embeddings\n",
    "- The bridge to Strand's Yoneda-based Probing Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Dray's final reflections\n",
    "final_letters = correspondence[correspondence['date'].str.startswith(('958', '960'))]\n",
    "\n",
    "print(\"Dray's Later Reflections:\\n\")\n",
    "for _, letter in final_letters.iterrows():\n",
    "    print(f\"Date: {letter['date']}\")\n",
    "    print(f\"Subject: {letter['subject']}\")\n",
    "    print(f\"\\n\\\"{letter['excerpt']}\\\"\\n\")\n",
    "    print(\"-\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course Summary\n",
    "\n",
    "In this course, we have learned:\n",
    "\n",
    "### Technical Concepts\n",
    "1. **The Archive as Category**: Access methods are objects, document flows are morphisms\n",
    "2. **Functors to Set**: Maps that assign sets to objects, functions to morphisms\n",
    "3. **Presheaves**: Contravariant functors that capture how observations \"pull back\"\n",
    "4. **Representable Functors**: Hom(A, -) captures the perspective from A\n",
    "5. **Embeddings as Values**: Each dimension is a probe response\n",
    "\n",
    "### Historical Timeline\n",
    "- Year 895: Dray born\n",
    "- Year 918: Multiple classification problem observed\n",
    "- Year 925: Archive-as-category insight\n",
    "- Year 926: Document functor theory proposed\n",
    "- Year 935: Representable perspective discovered\n",
    "- Year 942: Embedding-as-functor-values insight\n",
    "- Year 958: Definitive work published\n",
    "- Year 968: Dray dies\n",
    "\n",
    "### Connection to Modern ML\n",
    "Dray's document functors are exactly what modern transformers compute:\n",
    "- Each embedding dimension is a learned probe\n",
    "- Document embeddings are functor values\n",
    "- Attention weights relate to the Hom-functor perspective\n",
    "\n",
    "---\n",
    "\n",
    "## Next in the Series\n",
    "\n",
    "**Course 4: Natural Transformations (Gellen Tross)**\n",
    "\n",
    "If documents are functors, what are the relationships between documents? Gellen Tross will show us that **natural transformations** capture coherent shifts between representations.\n",
    "\n",
    "---\n",
    "\n",
    "*Part of the [Category Theory & LLMs Series](https://github.com/buildLittleWorlds)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
